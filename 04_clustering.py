''' 机器学习算法
聚类算法
'''

'''聚类
聚类（Clustering）是一种无监督学习方法，用于将数据集中的样本根据相似性分组或划分为多个子集，每个子集称为一个“簇”。在聚类分析中，目标是让同一个簇内的样本尽可能相似，而不同簇之间的样本尽可能不同。

### 1. **聚类的基本概念**

- **簇（Cluster）**：数据集中的样本根据某种相似性度量分组后的集合。簇内样本之间相似度高，簇间样本之间相似度低。
  
- **中心点（Centroid）**：在某些聚类算法中（如K均值），每个簇都有一个中心点，表示该簇中所有点的中心位置。

- **相似性度量**：用于衡量数据点之间相似度的度量方法，如欧几里得距离、曼哈顿距离、余弦相似度等。

### 2. **常见的聚类算法**

#### 1. **K均值聚类（K-Means Clustering）**

**概述**：
- K均值是最常用的聚类算法之一，目标是将数据分成 \(K\) 个簇，并最小化每个数据点与其所属簇的中心点之间的平方距离。

**步骤**：
1. 初始化：随机选择 \(K\) 个点作为初始中心点。
2. 分配簇：将每个数据点分配给离它最近的中心点。
3. 更新中心点：对每个簇，计算簇内所有点的均值，并将均值作为新的中心点。
4. 重复步骤 2 和 3，直到中心点不再发生变化或达到最大迭代次数。

**优点**：
- 简单且易于实现。
- 对大数据集有效，计算复杂度低。

**缺点**：
- 需要事先指定簇的数量 \(K\)。
- 对初始中心点敏感，不同的初始化可能导致不同的结果。
- 对噪声和离群点敏感。

**适用场景**：
- 当你需要明确分成 \(K\) 个簇且每个簇近似球形时，K均值效果较好。

#### 2. **层次聚类（Hierarchical Clustering）**

**概述**：
- 层次聚类通过构建一个树状结构（即树状图，dendrogram）来实现聚类分析。根据构建方式的不同，分为自底向上（凝聚型）和自顶向下（分裂型）两种方法。

**步骤（凝聚型）**：
1. 将每个数据点作为一个独立的簇。
2. 计算簇之间的相似性，并合并最相似的两个簇。
3. 重复步骤 2，直到所有簇合并成一个簇或达到预定的簇数。

**优点**：
- 不需要预先指定簇的数量。
- 可以生成多种层次的聚类结果，适用于探索数据的多层次结构。

**缺点**：
- 计算复杂度高，尤其是在大数据集上。
- 对噪声和离群点敏感。

**适用场景**：
- 适用于希望探索数据中潜在的层次结构时，如基因表达数据分析。

#### 3. **DBSCAN（Density-Based Spatial Clustering of Applications with Noise）**

**概述**：
- DBSCAN 是一种基于密度的聚类算法，能够识别任意形状的簇，并且可以自动识别离群点。

**步骤**：
1. 对于每个数据点，检查以其为中心、半径为 \( \epsilon \) 的邻域内的点数。
2. 如果邻域内的点数大于某个阈值（即最小样本数），则认为它是一个核心点，并将其邻域内的点划分到同一个簇。
3. 对于每个核心点，继续扩展其邻域内的簇，直到无法再扩展为止。
4. 不属于任何簇的点被标记为离群点。

**优点**：
- 能够发现任意形状的簇。
- 不需要指定簇的数量。
- 对噪声和离群点具有鲁棒性。

**缺点**：
- 对距离参数 \( \epsilon \) 和最小样本数较为敏感。
- 在密度变化较大的数据集中效果可能不佳。

**适用场景**：
- 适用于包含噪声或离群点的数据集，特别是簇的形状较为复杂时。

#### 4. **高斯混合模型（Gaussian Mixture Model, GMM）**

**概述**：
- GMM 是一种基于概率模型的聚类方法，它假设数据点是由若干个高斯分布混合而成，每个簇对应一个高斯分布。

**步骤**：
1. 初始化：为每个数据点随机分配一个初始的高斯分布参数。
2. E步：计算每个数据点属于每个高斯分布的概率。
3. M步：根据计算的概率，更新高斯分布的参数。
4. 重复E步和M步，直到参数收敛。

**优点**：
- 能处理具有不同形状的簇，并且能够对簇的重叠部分建模。
- 提供了簇所属的概率。

**缺点**：
- 对初始参数敏感，容易陷入局部最优解。
- 需要事先指定簇的数量。

**适用场景**：
- 当簇之间有重叠，且希望获得簇所属的概率时，GMM 是一个有效的选择。

### 3. **聚类算法的评价指标**

在无监督学习中，由于没有标签，聚类算法的评估较为困难，通常使用以下指标来评价聚类效果：

- **轮廓系数（Silhouette Coefficient）**：衡量数据点与同簇内其他点的相似性（簇内距离）与与最近邻簇的相似性（簇间距离）之间的差异。取值范围为[-1, 1]，值越大表示聚类效果越好。

- **轮廓系数公式**：
  \[
  s = \frac{b - a}{\max(a, b)}
  \]
  其中，\( a \) 是数据点与同簇内其他点的平均距离，\( b \) 是数据点与最近簇内点的平均距离。

- **CH指数（Calinski-Harabasz Index）**：度量簇内数据点的紧密性和簇间数据点的分离度。值越大表示聚类效果越好。

- **DB指数（Davies-Bouldin Index）**：度量簇内相似性与簇间相似性的比值。值越小表示聚类效果越好。

### 4. **聚类的应用场景**

- **客户分群**：在市场营销中，聚类可以用于根据购买行为或偏好将客户分成不同的群体，以便进行个性化营销。

- **图像分割**：在图像处理中，聚类可以用于将图像分割成不同的区域，如不同的颜色、纹理等。

- **文档聚类**：在自然语言处理领域，聚类可以用于将文档根据主题分成不同的类别，便于信息检索和分类。

- **异常检测**：在工业监控和金融领域，聚类可以用于检测数据中的异常点，帮助识别异常行为或故障。

### 总结

聚类是一种强大的无监督学习方法，能够帮助我们在没有标签的数据中发现隐藏的模式和结构。不同的聚类算法适用于不同的数据分布和应用场景，因此理解每种算法的特点和适用性，选择合适的算法是关键。通过结合聚类的评价指标，可以有效地评估和优化聚类结果，以更好地揭示数据中的潜在模式。
'''

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import calinski_harabasz_score, silhouette_score

X,y = make_blobs(n_samples=1000, n_features=2, centers=[[-1,-1], [0,0], [1,1], [2,2]], 
                                    cluster_std=[0.4, 0.2, 0.2, 0.2], random_state=42)
# plt.scatter(X[:,0], X[:,1], marker='o')
# plt.show()
# print(X.shape, y.shape)

y_pred = KMeans(n_clusters=4, random_state=42).fit_predict(X)
plt.scatter(X[:,0], X[:,1], c=y_pred, marker='o')
plt.show()

# 轮廓系数
silhouette = silhouette_score(X, y_pred)
print("Silhouette Score:", silhouette)

# CH指数
ch_score = calinski_harabasz_score(X, y_pred)
print("Calinski-Harabasz Index:", ch_score)